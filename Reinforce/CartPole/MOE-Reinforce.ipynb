{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38607566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/sanepunk/gymnax.git@main --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453525f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import gymnax\n",
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "import wandb\n",
    "import optax\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "key = jax.random.key(0)\n",
    "key, key_reset, key_act, key_step = jax.random.split(key, 4)\n",
    "\n",
    "env, env_params = gymnax.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallMoEExpert(nnx.Module):\n",
    "    \"\"\"Single expert in the MoE layer\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, rngs: nnx.Rngs, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.linear1 = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(hidden_dim, output_dim, rngs=rngs)\n",
    "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = jax.nn.leaky_relu(self.dropout(self.linear1(x)))\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallMoELayer(nnx.Module):\n",
    "    \"\"\"Very small MoE with only 2-3 experts\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts, rngs: nnx.Rngs, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # Create experts\n",
    "        self.experts = []\n",
    "        for i in range(num_experts):\n",
    "            expert_rngs = nnx.Rngs(i)\n",
    "            self.experts.append(SmallMoEExpert(input_dim, hidden_dim, output_dim, expert_rngs, dropout))\n",
    "\n",
    "        # Gating network\n",
    "        self.gate = nnx.Linear(input_dim, num_experts, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Compute gating weights\n",
    "        gate_logits = self.gate(x)  # [batch_size, seq_len, num_experts]\n",
    "        gate_weights = jax.nn.softmax(gate_logits, axis=-1)\n",
    "\n",
    "        # Get expert outputs\n",
    "        expert_outputs = []\n",
    "        for expert in self.experts:\n",
    "            expert_outputs.append(expert(x))  # [batch_size, seq_len, output_dim]\n",
    "\n",
    "        expert_outputs = jnp.stack(expert_outputs, axis=-2)  # [batch_size, seq_len, num_experts, output_dim]\n",
    "\n",
    "        # Weighted combination\n",
    "        gate_weights = gate_weights[..., None]  # [batch_size, seq_len, num_experts, 1]\n",
    "        output = jnp.sum(gate_weights * expert_outputs, axis=-2)  # [batch_size, seq_len, output_dim]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nnx.Module):\n",
    "    \"\"\"Simplified single-head attention\"\"\"\n",
    "    def __init__(self, embed_dim, rngs: nnx.Rngs):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query = nnx.Linear(embed_dim, embed_dim, rngs=rngs)\n",
    "        self.key = nnx.Linear(embed_dim, embed_dim, rngs=rngs)\n",
    "        self.value = nnx.Linear(embed_dim, embed_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        Q = self.query(x)  # [batch_size, seq_len, embed_dim]\n",
    "        K = self.key(x)    # [batch_size, seq_len, embed_dim]\n",
    "        V = self.value(x)  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # For self-attention with single timestep, we can simplify\n",
    "        if x.shape[1] == 1:  # seq_len == 1\n",
    "            # No need for complex attention computation with single timestep\n",
    "            # Just apply a learned transformation\n",
    "            output = V  # or could do Q + K + V or other combinations\n",
    "        else:\n",
    "            # Full attention computation for multiple timesteps\n",
    "            # Transpose the last two dimensions properly\n",
    "            K_T = jnp.swapaxes(K, -2, -1)  # [batch_size, embed_dim, seq_len]\n",
    "            scores = jnp.matmul(Q, K_T) / jnp.sqrt(self.embed_dim)\n",
    "            attention_weights = jax.nn.softmax(scores, axis=-1)\n",
    "            output = jnp.matmul(attention_weights, V)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cff4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEAttentionPolicy(nnx.Module):\n",
    "    def __init__(self, observation_space, action_space, rngs: nnx.Rngs, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = observation_space.shape[0]\n",
    "        self.hidden_size = 64\n",
    "        self.output_size = action_space.n\n",
    "\n",
    "        # Input projection to match hidden size\n",
    "        self.input_proj = nnx.Linear(self.input_size, self.hidden_size, rngs=rngs)\n",
    "\n",
    "        # First MoE layer (very small - 2 experts)\n",
    "        self.moe1 = SmallMoELayer(\n",
    "            input_dim=self.hidden_size,\n",
    "            hidden_dim=32,\n",
    "            output_dim=self.hidden_size,\n",
    "            num_experts=2,\n",
    "            rngs=rngs,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Simple attention layer\n",
    "        self.attention1 = SimpleAttention(self.hidden_size, rngs=rngs)\n",
    "\n",
    "        # Second MoE layer (very small - 2 experts)\n",
    "        self.moe2 = SmallMoELayer(\n",
    "            input_dim=self.hidden_size,\n",
    "            hidden_dim=32,\n",
    "            output_dim=self.hidden_size,\n",
    "            num_experts=2,\n",
    "            rngs=rngs,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.attention2 = SimpleAttention(self.hidden_size, rngs=rngs)\n",
    "\n",
    "        # Final layers\n",
    "        self.output = nnx.Linear(self.hidden_size, self.hidden_size, rngs=rngs)\n",
    "        self.mean_layer = nnx.Linear(self.hidden_size, self.output_size, rngs=rngs)\n",
    "        self.action_low = -2.0\n",
    "        self.action_high = 2.0\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for batched data\n",
    "        x: [batch_size, obs_dim] - flattened batch of all timesteps\n",
    "        \"\"\"\n",
    "        # Add sequence dimension for attention and MoE\n",
    "        if x.ndim == 2:\n",
    "            x = x[:, None, :]  # [batch_size, 1, obs_dim]\n",
    "\n",
    "        # Project input to hidden size\n",
    "        x = self.input_proj(x)  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # First MoE layer with residual connection\n",
    "        moe1_out = self.moe1(x)\n",
    "        x = jnp.add(x, moe1_out)  # Residual connection\n",
    "\n",
    "        # Attention layer with residual connection\n",
    "        attn_out = self.attention1(x)\n",
    "        x = jnp.add(x, attn_out)  # Residual connection\n",
    "\n",
    "        # Second MoE layer with residual connection\n",
    "        moe2_out = self.moe2(x)\n",
    "        x = jnp.add(x, moe2_out)  # Residual connection\n",
    "\n",
    "        attn_out = self.attention2(x)\n",
    "        x = jnp.add(x, attn_out)  # Residual connection\n",
    "        # Final layers\n",
    "        x = jax.nn.leaky_relu(self.output(x))\n",
    "        output = self.mean_layer(x)\n",
    "\n",
    "        # Remove sequence dimension if it was added\n",
    "        if output.shape[1] == 1:\n",
    "            output = output.squeeze(1)  # [batch_size, output_size]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def select_action(self, x, key):\n",
    "        # Ensure x has batch dimension for consistent processing\n",
    "        if x.ndim == 1:\n",
    "            x = x[None, :]  # Add batch dimension\n",
    "\n",
    "        logits = self(x)\n",
    "\n",
    "        # Remove batch dimension if it was added\n",
    "        if logits.shape[0] == 1:\n",
    "            logits = logits.squeeze(0)\n",
    "\n",
    "        return jax.random.categorical(key, logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630972ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"JAX-GYMNAX\", config={\n",
    "    \"env\": \"Cartpole\",\n",
    "    \"lr\": 1e-2,\n",
    "    \"gamma\": 0.99,\n",
    "    \"episodes\": 100,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def loss(model, obs, actions, returns):\n",
    "    log_logits = jax.nn.log_softmax(model(obs))\n",
    "    log_prob_taken = jnp.take_along_axis(log_logits, actions[:, None], axis=1).squeeze()\n",
    "    return -jnp.mean(log_prob_taken * returns)\n",
    "\n",
    "@nnx.jit\n",
    "def compute_returns(rewards, gamma):\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return jnp.array(returns)\n",
    "\n",
    "def train(env, env_params, model, episodes: int = 50, learning_rate=1e-3, gamma=0.99):\n",
    "    optimizer = nnx.Optimizer(\n",
    "        model,\n",
    "        optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),\n",
    "            optax.adam(learning_rate=learning_rate),\n",
    "        )\n",
    "    )\n",
    "    grad_func = nnx.value_and_grad(loss)\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    total_rewards = collections.deque(maxlen=100)\n",
    "\n",
    "    with tqdm(range(episodes)) as pbar:\n",
    "        for i in pbar:\n",
    "            batch_all_obs = []\n",
    "            batch_all_actions = []\n",
    "            batch_all_returns = []\n",
    "            # Using Batching for Gradient Stability\n",
    "            for _ in range(10):\n",
    "                episode_obs = []\n",
    "                episode_actions = []\n",
    "                episode_rewards = []\n",
    "\n",
    "                done = False\n",
    "                key, reset_key = jax.random.split(key)\n",
    "                obs, state = env.reset(reset_key, env_params)\n",
    "\n",
    "                while not done:\n",
    "                    key, action_key, step_key = jax.random.split(key, 3)\n",
    "                    action = model.select_action(obs, action_key)\n",
    "                    next_obs, state, reward, done, _ = env.step(step_key, state, action, env_params)\n",
    "\n",
    "                    episode_obs.append(obs)\n",
    "                    episode_actions.append(action)\n",
    "                    episode_rewards.append(reward)\n",
    "                    obs = next_obs\n",
    "\n",
    "                total_rewards.append(sum(episode_rewards))\n",
    "                returns = compute_returns(episode_rewards, gamma)\n",
    "\n",
    "                batch_all_obs.extend(episode_obs)\n",
    "                batch_all_actions.extend(episode_actions)\n",
    "                batch_all_returns.extend(returns)\n",
    "\n",
    "            final_obs = jnp.stack(batch_all_obs)\n",
    "            final_actions = jnp.array(batch_all_actions)\n",
    "            final_returns = jnp.array(batch_all_returns)\n",
    "\n",
    "            final_returns = (final_returns - jnp.mean(final_returns)) / (jnp.std(final_returns) + 1e-8)\n",
    "\n",
    "            value, grad = grad_func(model, final_obs, final_actions, final_returns)\n",
    "            optimizer.update(grad)\n",
    "\n",
    "            avg_reward = sum(total_rewards) / len(total_rewards)\n",
    "            wandb.log({\n",
    "                \"episodic_reward\": avg_reward,\n",
    "                \"global_step\": i,\n",
    "                \"loss\": value.item()\n",
    "            })\n",
    "            pbar.set_description(f\"Episode: {i}, Loss: {value.item():.4f}, Reward: {avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62db030",
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_attention_model = MoEAttentionPolicy(\n",
    "    env.observation_space(env_params),\n",
    "    env.action_space(env_params),\n",
    "    rngs=nnx.Rngs(0),\n",
    "    dropout=0.05\n",
    ")\n",
    "moe_attention_model.train()\n",
    "\n",
    "train(env, env_params, moe_attention_model, episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": ".langchain"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
