{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI-Q7vXaOGwN"
      },
      "outputs": [],
      "source": [
        "!pip install gymnax --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import gymnax\n",
        "from flax import nnx\n",
        "import jax.numpy as jnp\n",
        "import wandb\n",
        "import optax\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "\n",
        "key = jax.random.key(0)\n",
        "key, key_reset, key_act, key_step = jax.random.split(key, 4)\n",
        "\n",
        "env, env_params = gymnax.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "iMxJBSiqOL_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space(env_params).n\n",
        "env.observation_space(env_params).shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnz3ZP4QRFC0",
        "outputId": "7268f85e-e716-433a-a7c3-91cb24da4907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step()"
      ],
      "metadata": {
        "id": "lgHNNvcyXerh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nnx.Module):\n",
        "  def __init__(self, observation_space, action_space, rngs:nnx.Rngs):\n",
        "    super().__init__()\n",
        "    self.layer1 = nnx.Linear(observation_space.shape[0], 128, rngs = rngs)\n",
        "    self.layer2 = nnx.Linear(128, 128, rngs=rngs)\n",
        "\n",
        "    self.layer3 = nnx.Linear(128, action_space.n, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = jax.nn.relu(self.layer1(x))\n",
        "    x = jax.nn.relu(self.layer2(x))\n",
        "    return self.layer3(x)\n",
        "\n",
        "  def select_action(self, x, key):\n",
        "    logits = self(x)\n",
        "    return jax.random.categorical(key, logits)\n",
        "\n",
        "def loss(model, obs, actions, returns):\n",
        "    log_logits = jax.nn.log_softmax(model(obs))\n",
        "\n",
        "    log_prob_taken = jnp.take_along_axis(log_logits, actions[:, None], axis=1).squeeze()\n",
        "\n",
        "    return -jnp.mean(log_prob_taken * returns)\n",
        "\n",
        "def compute_returns(rewards, gamma):\n",
        "    R = 0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    return jnp.array(returns)\n",
        "\n",
        "model = Policy(env.observation_space(env_params), env.action_space(env_params), rngs=nnx.Rngs(0))\n",
        "wandb.init(project=\"JAX-GYMNAX\", config={\n",
        "    \"env\": \"Cartpole\",\n",
        "    \"lr\": 1e-2,\n",
        "    \"gamma\": 0.99,\n",
        "    \"episodes\": 500,\n",
        "})"
      ],
      "metadata": {
        "id": "gyt-CUQLQXaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, env_params, model, episodes: int = 50, learning_rate=1e-3, gamma=0.99):\n",
        "    optimizer = nnx.Optimizer(\n",
        "        model,\n",
        "        optax.chain(\n",
        "            optax.clip_by_global_norm(1.0),\n",
        "            optax.adam(learning_rate=learning_rate),\n",
        "        )\n",
        "    )\n",
        "    grad_func = nnx.value_and_grad(loss)\n",
        "    key = jax.random.PRNGKey(0)\n",
        "    total_rewards = collections.deque(maxlen=100)\n",
        "\n",
        "    with tqdm(range(episodes)) as pbar:\n",
        "        for i in pbar:\n",
        "            batch_all_obs = []\n",
        "            batch_all_actions = []\n",
        "            batch_all_returns = []\n",
        "            # Using Batching for Gradient Stability\n",
        "            for _ in range(10):\n",
        "                episode_obs = []\n",
        "                episode_actions = []\n",
        "                episode_rewards = []\n",
        "\n",
        "                done = False\n",
        "                key, reset_key = jax.random.split(key)\n",
        "                obs, state = env.reset(reset_key, env_params)\n",
        "\n",
        "                while not done:\n",
        "                    key, action_key, step_key = jax.random.split(key, 3)\n",
        "                    action = model.select_action(obs, action_key)\n",
        "                    next_obs, state, reward, done, _ = env.step(step_key, state, action, env_params)\n",
        "\n",
        "                    episode_obs.append(obs)\n",
        "                    episode_actions.append(action)\n",
        "                    episode_rewards.append(reward)\n",
        "                    obs = next_obs\n",
        "\n",
        "                total_rewards.append(sum(episode_rewards))\n",
        "                returns = compute_returns(episode_rewards, gamma)\n",
        "\n",
        "                batch_all_obs.extend(episode_obs)\n",
        "                batch_all_actions.extend(episode_actions)\n",
        "                batch_all_returns.extend(returns)\n",
        "\n",
        "            final_obs = jnp.stack(batch_all_obs)\n",
        "            final_actions = jnp.array(batch_all_actions)\n",
        "            final_returns = jnp.array(batch_all_returns)\n",
        "\n",
        "            final_returns = (final_returns - jnp.mean(final_returns)) / (jnp.std(final_returns) + 1e-8)\n",
        "\n",
        "            value, grad = grad_func(model, final_obs, final_actions, final_returns)\n",
        "            optimizer.update(grad)\n",
        "\n",
        "            avg_reward = sum(total_rewards) / len(total_rewards)\n",
        "            wandb.log({\n",
        "                \"episodic_reward\": avg_reward,\n",
        "                \"global_step\": i,\n",
        "                \"loss\": value.item()\n",
        "            })\n",
        "            pbar.set_description(f\"Episode: {i}, Loss: {value.item():.4f}, Reward: {avg_reward:.2f}\")"
      ],
      "metadata": {
        "id": "XBSvFEOZc2vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(env, env_params, model, episodes = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMUux8dabRy8",
        "outputId": "43fc95e0-5698-427a-bfa2-1e4567f7ff26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode: 99, Loss: -0.0097, Reward: 471.87: 100%|██████████| 100/100 [39:06<00:00, 23.47s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import serialization\n",
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save(f'/content/model_state', state)"
      ],
      "metadata": {
        "id": "GYKmj2m7saWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = '/content/model_state'\n",
        "destination_dir = '/content/drive/MyDrive/RL_MODELS/'\n",
        "\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "shutil.copytree(source_dir, os.path.join(destination_dir, os.path.basename(source_dir)), dirs_exist_ok=True)\n",
        "\n",
        "print(f\"Model state copied from {source_dir} to {destination_dir}\")"
      ],
      "metadata": {
        "id": "QcF2MQuCtlz3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}